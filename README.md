1. Project
The goal of the project is to build a speaker identification (SID) system which identifies the speaker of a given speech utterance. We will provide the classification engine and database. Once you run the baseline system and examine the provided MATLAB code and data structure (Instructions Section), you will understand the following parts more easily.
2. Data:
The data is the vowel /a/ for 5 adult males and 5 females. There are 8 examples for each speaker. The data were collected on 3 different days. The training data contains 6 utterances per speaker and the testing data has 2 utterances per speaker. The first test set consists of clean utterances while the second set was generated by adding babble noise at 10dB SNR. All the data was original sampled at 22050Hz, and since you are required to do the analysis within 4kHz bandwidth, therefore the files are down sampled to 8kHz before feature extraction.
Training and test data are stored in cell structures named TrainCell and TestCell, respectively; the speaker labels are stored as arrays named TrainLabel and TestLabel, respectively. Each cell of training and test cells contains a waveform utterance represented in a vector format. The speaker IDs are represented by integers 1 - 10.
3. System Overview
There are 2 components you may explore to develop a noise robust SID system.
1) Feature Extraction:
You should think carefully about the kind of spectral representation you will use, frame length, use of dynamic spectral information, DFT, LPC, time-varying features, etc. Try different features that you think will represent speaker information and find the best one which gives you best results.
2) Classifier: The classifier used for the baseline system is GMM based. Each speaker is modeled using 6 diagonal- covariance Gaussian components. The speaker classification label is assigned using the maximum likelihood criterion. Developing robust frontend features for a given classification engine is the main focus of this project.
4. Instructions
1) Run mainProgram.m. For your first-time running the program, you should execute the code and make sure each part works before executing the next one.
Part1: you need to load the data and add the “voicebox” into the path. You have two options: a) clean train data with clean test data, and b) clean train data with 10db babble noise test data. Choose one of them.
Part2: for the training part, change the code in the “FEATURE CHOICES” part to try different features. In the code, LPC features are provided as the baseline (denoted as function LPCbasic).
Part3: test the 20 files in the test cell to get identification accuracy.
2) You should get results similar to the ones below (for the clean training/clean testing case):
Training a model for Speaker 1 Training a model for Speaker 2 Training a model for Speaker 3 ...
Training a model for Speaker 10
--> Start testing --> Accuracy: 70 %
You should probably do most of your implementation within the feature extraction section. The accuracy for the clean training/noisy testing is 60 percent.
5. Useful MATLAB functions/operations
Several functions might be useful. Some are built-in in MATLAB codes while others can be found online such as Voice Box (http://www.ee.ic.ac.uk/hp/staff/dmb/voicebox/voicebox.html). In the provided code, Voice Box is added in the first part.
6. Baseline system
1) Training: for each speaker ID, each utterance is used to extract features. The features of each frame for each utterance are concatenated to form a feature array, which is then used to train a GMM for the speaker. The GMM object is stored in a cell called gmmCell.
2) Testing: for a given utterance, the features are extracted. For each speaker GMM, the summation of log likelihoods are computed from the feature vectors. The speaker label is assigned to the model that gives the maximum likelihood.
3) Baseline results: For the clean test dataset, the accuracy using LPC gives you around 70% accuracy. For the 10db Babble noise test dataset, the accuracy using LPC gives you around 60% accuracy.
7. Other software: Voicesauce is software developed at UCLA which allows you to extract features which might be helpful for speaker identification. You can download the software from: http://www.seas.ucla.edu/spapl/voicesauce/
8. Oral Presentations
There will be oral presentations by the different teams describing their work. Presentations should be planned by the team as a group.
9. Report and Code
The report (one per group) should include:
• Introduction (what is the problem/why is it important).
• Background (literature survey).
• Project Description (methodology, implementation, results, etc.).
• Summary and Discussion (also, ideas for future work).
The report should be no longer than 6-pages and have the same format as ICASSP. Figures and flowcharts generally help clarify the text.
Code should be turned in on the day of the presentation. To evaluate the robustness of your system, we will use another dataset which contain different speakers to determine the classification accuracy. Comments at the beginning of each function should describe what the function intends to do. The final report may be turned in by the following Monday.
10. Reference
1.
Reynolds, Douglas A., and Richard C. Rose. "Robust text-independent speaker identification using Gaussian
mixture speaker models." Speech and Audio Processing, IEEE Transactions on 3.1 (1995): 72-83.
2. Kinnunen, Tomi, and Haizhou Li. "An overview of text-independent speaker recognition: from features to
supervectors." Speech communication 52.1 (2010): 12-40.
3. Harish Arsikere, H.A. Gupta and Abeer Alwan, "Speaker recognition via fusion of subglottal features and
MFCCs", Interspeech 2014.
